{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e977990-8c4e-412d-934d-b2e4a3efb319",
   "metadata": {},
   "source": [
    "# Acquiring Article Data\n",
    "\n",
    "In this notebook we'll go through the process of acquiring all the article data that will be used in this project. The steps outlined here will allow us to replicate the process of acquiring the data, but new articles are published every day making it difficult to fully reproduce the process of acquiring the same dataset that will be used in exploration and model training for this project. Nonetheless the steps will be outlined here for reference. A .csv file will be provided containing the data that is used for analysis and modeling for reproducibility.\n",
    "\n",
    "## Imports\n",
    "\n",
    "These are all the modules that we'll need to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5ede9c-d561-44b8-b77c-3239a2f0ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need to add the modules directory path to sys.path since this is where all the .py files are located.\n",
    "\n",
    "import sys\n",
    "sys.path.append('../modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ae2493-c3f1-4ef0-b964-a7648ff506f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The Medium API will be used to acquire data.\n",
    "# Medium API Documentation: https://github.com/weeping-angel/medium-api\n",
    "\n",
    "from medium_api import Medium\n",
    "\n",
    "# The News API will be used to acquire data from a large variety of sources.\n",
    "# News API Documentation: https://newsapi.org/docs\n",
    "\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "# We need requests and Beautiful Soup in order to scrape web data.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# We need an API key to use the Medium API. This can be acquired from rapidapi.com.\n",
    "# It is required to create an account and subscribe to the Medium API.\n",
    "# https://rapidapi.com/nishujain199719-vgIfuFHZxVZ/api/medium2/pricing\n",
    "\n",
    "from env import medium_api_key, news_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db6ec3-9212-460f-b8b1-b69d180d87ec",
   "metadata": {},
   "source": [
    "## Acquire Article Data Using Medium API\n",
    "\n",
    "In this section we'll cover some of the functionality provided by the Medium API. We'll discover how to use it and what can and can't be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722b0059-2c6c-48be-85c9-c82c6a433e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to create the Medium object with our API key, this will allow us to make calls to the API.\n",
    "\n",
    "medium = Medium(medium_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "320a11c1-6b4e-40cf-9c5b-62cb924b890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try getting the latestposts for a given topic. The latestposts function only allows specifying a \n",
    "# topic.\n",
    "\n",
    "latestposts = medium.latestposts(topic_slug = 'artificial-intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebe4042a-dc1c-4bc6-8955-0a36433a20a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '14f1ac7ec113',\n",
       " 'tags': ['smart-speaker',\n",
       "  'smartspeakersmarketshare',\n",
       "  'telegram',\n",
       "  'telegram-bot',\n",
       "  'voice-assistant'],\n",
       " 'claps': 0,\n",
       " 'last_modified_at': '2022-07-04 20:53:06',\n",
       " 'published_at': '2022-07-04 20:53:06',\n",
       " 'url': 'https://convcomp.it/could-telegram-be-a-competitor-of-voice-assistants-like-amazon-alexa-or-google-assistant-14f1ac7ec113',\n",
       " 'image_url': 'https://miro.medium.com/1*TyS9LOsSoh-506qWQURRlw.jpeg',\n",
       " 'lang': 'en',\n",
       " 'publication_id': 'e9c948ff6ebd',\n",
       " 'title': 'Could Telegram be a competitor of voice assistants, like Amazon Alexa or Google Assistant?',\n",
       " 'word_count': 2285,\n",
       " 'reading_time': 8.822641509434,\n",
       " 'voters': 0,\n",
       " 'topics': ['artificial-intelligence'],\n",
       " 'subtitle': 'An open letter to Pavel Durov, containing some change requests to enable voice integration into Telegram bots ecosystem',\n",
       " 'author': '452c0445f9d5'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's look at the info for the first article in this list.\n",
    "\n",
    "latestposts.articles[0].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dac407e5-218b-4888-b6be-ec6132f6bd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'fe99c68643a6',\n",
       " 'tags': ['metaverse', 'machine-learning', 'strategy', 'marketing', 'law'],\n",
       " 'claps': 0,\n",
       " 'last_modified_at': '2022-07-04 20:05:43',\n",
       " 'published_at': '2022-07-04 19:58:46',\n",
       " 'url': 'https://medium.com/@MetaverseLaw/metaverse-law-trends-via-machine-learning-fe99c68643a6',\n",
       " 'image_url': 'https://miro.medium.com/1*rta8P5wpi5-75cERsoJa2g.png',\n",
       " 'lang': 'en',\n",
       " 'publication_id': '*Self-Published*',\n",
       " 'title': 'Metaverse Law Trends via Machine Learning',\n",
       " 'word_count': 3814,\n",
       " 'reading_time': 15.742452830189,\n",
       " 'voters': 0,\n",
       " 'topics': ['artificial-intelligence'],\n",
       " 'subtitle': 'If you’re interested only in the machine learning discussion, please skip to the Machine Learning section.',\n",
       " 'author': '1506fccbdfe3'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the second article.\n",
    "\n",
    "latestposts.articles[1].info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b60b30-52bd-42df-818a-1bab25045f62",
   "metadata": {},
   "source": [
    "There was a brief delay when grabbing the info for the articles. This makes me wonder if the API is lazy and makes the call only when necessary (in this case when we call info). If this is the case then using the API, on the free tier at least, will not be possible since we will likely expend the allotted monthly API calls very rapidly. Looking at the API usage stats (at RapidAPI) it seems like this is in fact the case.\n",
    "\n",
    "The documentation shows there is a fetch_articles function that may solve this issue. Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13cbac4e-c52b-4d99-a355-8d684c66cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the article data using fetch_articles, this function doesn't return anything, but will instead populate\n",
    "# the latestposts.articles list.\n",
    "\n",
    "medium.fetch_articles(latestposts.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eab80362-54b1-4855-bb79-7352753bc631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(latestposts.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fe6a927-c1a7-42b6-b036-c804c2b2d421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '14f1ac7ec113',\n",
       " 'tags': ['smart-speaker',\n",
       "  'smartspeakersmarketshare',\n",
       "  'telegram',\n",
       "  'telegram-bot',\n",
       "  'voice-assistant'],\n",
       " 'claps': 0,\n",
       " 'last_modified_at': '2022-07-04 20:53:06',\n",
       " 'published_at': '2022-07-04 20:53:06',\n",
       " 'url': 'https://convcomp.it/could-telegram-be-a-competitor-of-voice-assistants-like-amazon-alexa-or-google-assistant-14f1ac7ec113',\n",
       " 'image_url': 'https://miro.medium.com/1*TyS9LOsSoh-506qWQURRlw.jpeg',\n",
       " 'lang': 'en',\n",
       " 'publication_id': 'e9c948ff6ebd',\n",
       " 'title': 'Could Telegram be a competitor of voice assistants, like Amazon Alexa or Google Assistant?',\n",
       " 'word_count': 2285,\n",
       " 'reading_time': 8.822641509434,\n",
       " 'voters': 0,\n",
       " 'topics': ['artificial-intelligence'],\n",
       " 'subtitle': 'An open letter to Pavel Durov, containing some change requests to enable voice integration into Telegram bots ecosystem',\n",
       " 'author': '452c0445f9d5'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's try getting the info for a few articles again.\n",
    "\n",
    "latestposts.articles[0].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc59f88c-536f-4597-9b53-c4db9a842359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '65de5d04c475',\n",
       " 'tags': ['artificial-intelligence',\n",
       "  'machine-learning',\n",
       "  'data-science',\n",
       "  'automation',\n",
       "  'future'],\n",
       " 'claps': 250,\n",
       " 'last_modified_at': '2022-07-04 13:10:28',\n",
       " 'published_at': '2022-07-04 12:26:24',\n",
       " 'url': 'https://medium.com/gdg-vit/ai-automation-and-the-future-of-workplaces-65de5d04c475',\n",
       " 'image_url': 'https://miro.medium.com/1*b0ig8NZnu7GBr2tZG0xnsg.png',\n",
       " 'lang': 'en',\n",
       " 'publication_id': '7ebddf9721d',\n",
       " 'title': 'AI, Automation, and the Future of Workplaces',\n",
       " 'word_count': 1590,\n",
       " 'reading_time': 6.8333333333333,\n",
       " 'voters': 5,\n",
       " 'topics': ['artificial-intelligence'],\n",
       " 'subtitle': 'Introduction',\n",
       " 'author': '15c03e4e1b20'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latestposts.articles[10].info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46918026-f0d5-45d3-9325-1d4d4f931c67",
   "metadata": {},
   "source": [
    "There is no longer a delay when getting the article info, but the API usage stats (at RapidAPI) confirm that running the fetch_articles functions made 25 API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648fd701-9dc1-4ad1-8d0a-13e5c4c2f48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '3487bb252780',\n",
       " 'tags': ['coding-challenge',\n",
       "  'google',\n",
       "  'python',\n",
       "  'getting-started',\n",
       "  'programming'],\n",
       " 'claps': 10,\n",
       " 'last_modified_at': '2022-07-05 20:03:53',\n",
       " 'published_at': '2022-07-05 19:19:47',\n",
       " 'url': 'https://towardsdatascience.com/google-foobar-challenge-level-1-3487bb252780',\n",
       " 'image_url': 'https://miro.medium.com/0*RX1cyqxIba1gKt1b',\n",
       " 'lang': 'en',\n",
       " 'publication_id': '7f60cf5620c9',\n",
       " 'title': 'Google Foobar Challenge: Level 1',\n",
       " 'word_count': 1243,\n",
       " 'reading_time': 5.0738993710692,\n",
       " 'voters': 2,\n",
       " 'topics': ['data-science', 'programming'],\n",
       " 'subtitle': 'An intro to the secretive coding challenge and a breakdown of the problems',\n",
       " 'author': '94ed6e69690'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One last thing, let's try getting the information for a specific article which we know the URL of.\n",
    "# https://towardsdatascience.com/google-foobar-challenge-level-1-3487bb252780\n",
    "# The article id is the series of characters at the end of the URL after the article title. 3487bb252780\n",
    "\n",
    "article = medium.article(article_id = '3487bb252780')\n",
    "article.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c5a31-9acf-4646-bc08-5fcaff453362",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    "Using the API would definitely make things easier, but it comes with some very significant limitations. First from the work done above we can see that for each article we collect data on the API will make a call. This means that if, for instance, we wanted to get the latest 25 articles for 5 different topics this would amount to 125 API calls which is half the monthly allotment of 250. Even using a smarter algorithm to ensure we don't make more calls than necessary we would still probably expend the monthly free tier allotment within a week.\n",
    "\n",
    "Switching to a paid tier is an option. The next tier up would cost 4.99 per month and give 1,250 API calls per month. This might be enough, but it comes with the downside of having to pay for the API. Not to mention that it might possibly still not be enough calls as the application begins to scale to a larger size. The API with the free tier would probably be good enough if this application was kept to a small scope, such as only pulling article data for 2 or 3 different topics. This would not make for a very useful application though so we may need to explore alternative solutions.\n",
    "\n",
    "It might be worthwhile to design and build a package that utilizes the API just in case later on it is discovered that the API is necessary.\n",
    "\n",
    "## Acquiring Article Data Through Web Scraping\n",
    "\n",
    "For scraping the data we need there are a few things to keep in mind.\n",
    "\n",
    "1. It is possible to structure the URL in such a way that we can get the latest articles, but without JavaScript enabled we only get 10 articles. By simply counting the number of articles published in a single day for a single topic we can expect that for most topics and publications there will likely be more than 10 articles published per day. This means we would either need to run the web scraping script multiple times a day or use a dynamic web scraping library like Selenium. I know from experience that Selenium does not play well with Crontab so this could be problematic.\n",
    "2. Getting the latest articles for a publication shouldn't be too much of an issue. Getting the latest articles for a specific topic could be problematic. Searching for a specific through the main medium site provides unusual results (the articles don't seem to be in order of most recent, articles of varying related topics seem to be returned, I also seem to get different results for Chrome and Safari).\n",
    "\n",
    "At this point I believe it is best to move forward with the solution that will be best given the limitations present. So we will only pull the latest articles for given publications and not for specific topics (at least not until this weird issue with the return results is fixed). To accomplish this task we will need requests and Beautiful Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d71f943-2a1f-4553-ab1c-50559b7efb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll attempt to get the HTML for the latest articles page from towardsdatascience.com\n",
    "\n",
    "url = 'https://towardsdatascience.com/latest'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a399a1-de16-4c2e-b00f-20627096dbe5",
   "metadata": {},
   "source": [
    "For each article I would like to gather the following data:\n",
    "- Title\n",
    "- Subtitle\n",
    "- Date\n",
    "- Read Time\n",
    "- Author\n",
    "- Publication\n",
    "- URL\n",
    "- Article Intro\n",
    "\n",
    "I'll start by first getting all the article URLs on the latest article page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a193f4-2e1a-4550-9206-e2d17827160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URLs for all the latest articles.\n",
    "\n",
    "links = soup.find_all('a', title = 'Latest stories published on Towards Data Science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3cdce26-486d-4412-846a-58f1d86b2c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f31dd3c-a6f3-488f-b0ec-b2bd0b00f1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://towardsdatascience.com/navigating-mlops-dc2a242ef7ed?source=---------0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b673d4c-7523-4424-bdb5-4cda3e03f1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://towardsdatascience.com/ai-can-now-play-minecraft-and-is-a-step-closer-to-navigate-the-world-1f19cfe37ef?source=---------1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[1]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf7dbd6b-4012-4c17-adfa-f1689fd1093c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://towardsdatascience.com/navigating-mlops-dc2a242ef7ed',\n",
       " 'https://towardsdatascience.com/ai-can-now-play-minecraft-and-is-a-step-closer-to-navigate-the-world-1f19cfe37ef',\n",
       " 'https://towardsdatascience.com/calculating-closest-landmass-between-two-points-on-earth-214f73b48fdc',\n",
       " 'https://towardsdatascience.com/visualizing-cpu-memory-and-gpu-utilities-with-python-8028d859c2b0',\n",
       " 'https://towardsdatascience.com/choosing-neural-networks-over-n-gram-models-for-natural-language-processing-156ea3a57fc',\n",
       " 'https://towardsdatascience.com/numpy-ufuncs-the-magic-behind-vectorized-functions-8cc3ba56aa2c',\n",
       " 'https://towardsdatascience.com/block-recurrent-transformer-lstm-and-transformer-combined-ec3e64af971a',\n",
       " 'https://towardsdatascience.com/5-ideas-to-create-new-features-from-polygons-f8f902f5ad8f',\n",
       " 'https://towardsdatascience.com/how-to-use-customer-lifetime-value-ltv-for-data-driven-transformation-6f12596df943',\n",
       " 'https://towardsdatascience.com/self-supervised-transformer-models-bert-gpt3-mum-and-paml-2b5e29ea0c26']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's put all the URLs into a list.\n",
    "# I'll also strip the ?source= stuff at the end of the URL.\n",
    "\n",
    "urls = [link['href'].split('?source')[0] for link in links]\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1a0d8-a496-42ba-a71c-ab05ce660bb4",
   "metadata": {},
   "source": [
    "Next we can try to gather all the data we need from a given article by scraping the webpage for that URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32f4f22a-4382-4cd1-aa79-9cc8abe0529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(urls[1])\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee6f7d1e-229b-4de2-b648-70bf240fd748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alberto Romero'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the author's name by searching for the class pw-author in the HTML. The author's name is then \n",
    "# three <div>s and a <a> below that.\n",
    "\n",
    "author_name = soup.find('div', class_ = 'pw-author').div.div.div.a.text\n",
    "author_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dc800d6-432e-4a79-a5f7-912aad198748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jul 6'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the publishing date by searching for the class pw-published-date in the HTML. The publishing \n",
    "# date is then one <span> below that.\n",
    "\n",
    "date = soup.find('p', class_ = 'pw-published-date').span.text\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c890abf0-8ce5-4d91-afec-c8cca1747698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7 min read'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the read time by searching for the class pw-reading-time in the HTML.\n",
    "\n",
    "read_time = soup.find('div', class_ = 'pw-reading-time').text\n",
    "read_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e767cd01-3e13-41f4-b965-5c4acf047c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI Can Now Play Minecraft — A Step Closer to Navigate the World'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the title by searching for the class pw-post-title in the HTML.\n",
    "\n",
    "title = soup.find('h1', class_ = 'pw-post-title').text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52cf12cf-b30d-4440-be6a-3d9198c4bc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The beginning of open-ended AI'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the subtitle by searching for the class pw-subtitle-paragraph in the HTML.\n",
    "\n",
    "subtitle = soup.find('h2', class_ = 'pw-subtitle-paragraph').text\n",
    "subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c0923c2-c176-4541-88b4-75ba3ad55064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After building impressive models in language processing (GPT-3) and text-to-image generation (DALL·E 2), OpenAI is now facing an arguably greater challenge: open-ended action. In the great task of solving so-called artificial general intelligence (AGI), they realize language and vision aren’t the only domains in which AI should excel. GPT-3 and DALL·E 2 are extremely good at what they do, but as powerful as they are they remain constrained within the limited boundaries of their virtual worlds.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the article intro by searching for the class \n",
    "\n",
    "article_intro = soup.find('p', class_ = 'pw-post-body-paragraph').text\n",
    "article_intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d0366-feb3-4cb7-b9b8-1b8d3919577f",
   "metadata": {},
   "source": [
    "Lastly, URL and publication are both values that we would already have. The URL would have been scraped from the latest articles page and the publication will have been provided with the original URL. Now we are able to put all this information in a dictionary and add it to a pandas dataframe. Let's try this out, but run it for all the links we scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c7ea99c-240d-4314-9047-ffd8bfbbc659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>article_intro</th>\n",
       "      <th>date</th>\n",
       "      <th>read_time</th>\n",
       "      <th>url</th>\n",
       "      <th>publication_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christian Freischlag</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Navigating MLOps in 2022</td>\n",
       "      <td>Data Science in production: experience, tools ...</td>\n",
       "      <td>MLOps has established itself as an independent...</td>\n",
       "      <td>Jul 6</td>\n",
       "      <td>6 min read</td>\n",
       "      <td>https://towardsdatascience.com/navigating-mlop...</td>\n",
       "      <td>https://towardsdatascience.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alberto Romero</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>AI Can Now Play Minecraft — A Step Closer to N...</td>\n",
       "      <td>The beginning of open-ended AI</td>\n",
       "      <td>After building impressive models in language p...</td>\n",
       "      <td>Jul 6</td>\n",
       "      <td>7 min read</td>\n",
       "      <td>https://towardsdatascience.com/ai-can-now-play...</td>\n",
       "      <td>https://towardsdatascience.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrew Hershy</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Calculating Closest Landmass Between Two Point...</td>\n",
       "      <td>Python script to identify midpoint and closest...</td>\n",
       "      <td>I made some significant updates to an existing...</td>\n",
       "      <td>Jul 6</td>\n",
       "      <td>5 min read</td>\n",
       "      <td>https://towardsdatascience.com/calculating-clo...</td>\n",
       "      <td>https://towardsdatascience.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bharath K</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Visualizing CPU, Memory, And GPU Utilities wit...</td>\n",
       "      <td>Analyzing CPU, memory usage, and GPU component...</td>\n",
       "      <td>When you are indulged in programming, you are ...</td>\n",
       "      <td>Jul 6</td>\n",
       "      <td>7 min read</td>\n",
       "      <td>https://towardsdatascience.com/visualizing-cpu...</td>\n",
       "      <td>https://towardsdatascience.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benjamin McCloskey</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Choosing Neural Networks over N-Gram Models fo...</td>\n",
       "      <td>Today we will look at the strengths of using R...</td>\n",
       "      <td>Traditional learning models transform text fro...</td>\n",
       "      <td>Jul 6</td>\n",
       "      <td>8 min read</td>\n",
       "      <td>https://towardsdatascience.com/choosing-neural...</td>\n",
       "      <td>https://towardsdatascience.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Diego Barba</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>NumPy ufuncs — The Magic Behind Vectorized Fun...</td>\n",
       "      <td>Learn about NumPy universal functions (ufuncs)...</td>\n",
       "      <td>Have you ever wondered about the origin of Num...</td>\n",
       "      <td>Jul 6</td>\n",
       "      <td>7 min read</td>\n",
       "      <td>https://towardsdatascience.com/numpy-ufuncs-th...</td>\n",
       "      <td>https://towardsdatascience.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nikos Kafritsas</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Block-Recurrent Transformer: LSTM and Transfor...</td>\n",
       "      <td>A powerful model that combines the best of bot...</td>\n",
       "      <td>The vanilla Transformer is no longer the all-m...</td>\n",
       "      <td>Jul 6</td>\n",
       "      <td>12 min read</td>\n",
       "      <td>https://towardsdatascience.com/block-recurrent...</td>\n",
       "      <td>https://towardsdatascience.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Leonie Monigatti</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>5 Ideas to Create New Features from Polygons</td>\n",
       "      <td>How to Get the Area and Other Features From a ...</td>\n",
       "      <td>Polygon data can be useful in various applicat...</td>\n",
       "      <td>Jul 6</td>\n",
       "      <td>6 min read</td>\n",
       "      <td>https://towardsdatascience.com/5-ideas-to-crea...</td>\n",
       "      <td>https://towardsdatascience.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lak Lakshmanan</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>How to use Customer Lifetime Value (LTV) for d...</td>\n",
       "      <td>Use LTV to set goals for your business and dev...</td>\n",
       "      <td>If your business is undertaking a data-driven ...</td>\n",
       "      <td>Jul 6</td>\n",
       "      <td>12 min read</td>\n",
       "      <td>https://towardsdatascience.com/how-to-use-cust...</td>\n",
       "      <td>https://towardsdatascience.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Siwei Causevic</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>Self-supervised Transformer Models — BERT, GPT...</td>\n",
       "      <td>None</td>\n",
       "      <td>In the early days, NLP systems are mostly rule...</td>\n",
       "      <td>Jul 6</td>\n",
       "      <td>6 min read</td>\n",
       "      <td>https://towardsdatascience.com/self-supervised...</td>\n",
       "      <td>https://towardsdatascience.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author           publication  \\\n",
       "0  Christian Freischlag  Towards Data Science   \n",
       "1        Alberto Romero  Towards Data Science   \n",
       "2         Andrew Hershy  Towards Data Science   \n",
       "3             Bharath K  Towards Data Science   \n",
       "4    Benjamin McCloskey  Towards Data Science   \n",
       "5           Diego Barba  Towards Data Science   \n",
       "6       Nikos Kafritsas  Towards Data Science   \n",
       "7      Leonie Monigatti  Towards Data Science   \n",
       "8        Lak Lakshmanan  Towards Data Science   \n",
       "9        Siwei Causevic  Towards Data Science   \n",
       "\n",
       "                                               title  \\\n",
       "0                           Navigating MLOps in 2022   \n",
       "1  AI Can Now Play Minecraft — A Step Closer to N...   \n",
       "2  Calculating Closest Landmass Between Two Point...   \n",
       "3  Visualizing CPU, Memory, And GPU Utilities wit...   \n",
       "4  Choosing Neural Networks over N-Gram Models fo...   \n",
       "5  NumPy ufuncs — The Magic Behind Vectorized Fun...   \n",
       "6  Block-Recurrent Transformer: LSTM and Transfor...   \n",
       "7       5 Ideas to Create New Features from Polygons   \n",
       "8  How to use Customer Lifetime Value (LTV) for d...   \n",
       "9  Self-supervised Transformer Models — BERT, GPT...   \n",
       "\n",
       "                                            subtitle  \\\n",
       "0  Data Science in production: experience, tools ...   \n",
       "1                     The beginning of open-ended AI   \n",
       "2  Python script to identify midpoint and closest...   \n",
       "3  Analyzing CPU, memory usage, and GPU component...   \n",
       "4  Today we will look at the strengths of using R...   \n",
       "5  Learn about NumPy universal functions (ufuncs)...   \n",
       "6  A powerful model that combines the best of bot...   \n",
       "7  How to Get the Area and Other Features From a ...   \n",
       "8  Use LTV to set goals for your business and dev...   \n",
       "9                                               None   \n",
       "\n",
       "                                       article_intro   date    read_time  \\\n",
       "0  MLOps has established itself as an independent...  Jul 6   6 min read   \n",
       "1  After building impressive models in language p...  Jul 6   7 min read   \n",
       "2  I made some significant updates to an existing...  Jul 6   5 min read   \n",
       "3  When you are indulged in programming, you are ...  Jul 6   7 min read   \n",
       "4  Traditional learning models transform text fro...  Jul 6   8 min read   \n",
       "5  Have you ever wondered about the origin of Num...  Jul 6   7 min read   \n",
       "6  The vanilla Transformer is no longer the all-m...  Jul 6  12 min read   \n",
       "7  Polygon data can be useful in various applicat...  Jul 6   6 min read   \n",
       "8  If your business is undertaking a data-driven ...  Jul 6  12 min read   \n",
       "9  In the early days, NLP systems are mostly rule...  Jul 6   6 min read   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://towardsdatascience.com/navigating-mlop...   \n",
       "1  https://towardsdatascience.com/ai-can-now-play...   \n",
       "2  https://towardsdatascience.com/calculating-clo...   \n",
       "3  https://towardsdatascience.com/visualizing-cpu...   \n",
       "4  https://towardsdatascience.com/choosing-neural...   \n",
       "5  https://towardsdatascience.com/numpy-ufuncs-th...   \n",
       "6  https://towardsdatascience.com/block-recurrent...   \n",
       "7  https://towardsdatascience.com/5-ideas-to-crea...   \n",
       "8  https://towardsdatascience.com/how-to-use-cust...   \n",
       "9  https://towardsdatascience.com/self-supervised...   \n",
       "\n",
       "                  publication_url  \n",
       "0  https://towardsdatascience.com  \n",
       "1  https://towardsdatascience.com  \n",
       "2  https://towardsdatascience.com  \n",
       "3  https://towardsdatascience.com  \n",
       "4  https://towardsdatascience.com  \n",
       "5  https://towardsdatascience.com  \n",
       "6  https://towardsdatascience.com  \n",
       "7  https://towardsdatascience.com  \n",
       "8  https://towardsdatascience.com  \n",
       "9  https://towardsdatascience.com  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'author' : [],\n",
    "    'publication' : [],\n",
    "    'title' : [],\n",
    "    'subtitle' : [],\n",
    "    'article_intro' : [],\n",
    "    'date' : [],\n",
    "    'read_time' : [],\n",
    "    'url' : [],\n",
    "    'publication_url' : []\n",
    "})\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    article_info = {}\n",
    "    \n",
    "    article_info['author'] = [soup.find('div', class_ = 'pw-author').div.div.div.a.text]\n",
    "    article_info['publication'] = ['Towards Data Science']\n",
    "    article_info['title'] = [soup.find('h1', class_ = 'pw-post-title').text]\n",
    "    article_info['article_intro'] = [soup.find('p', class_ = 'pw-post-body-paragraph').text]\n",
    "    article_info['date'] = [soup.find('p', class_ = 'pw-published-date').span.text]\n",
    "    article_info['read_time'] = [soup.find('div', class_ = 'pw-reading-time').text]\n",
    "    article_info['url'] = [url]\n",
    "    article_info['publication_url'] = ['https://towardsdatascience.com']\n",
    "    \n",
    "    # Sometimes articles don't have a subtitle so we must check if there is a subtitle and \n",
    "    # set the subtitle to an empty string if one doesn't exist.\n",
    "    if (subtitle := soup.find('h2', class_ = 'pw-subtitle-paragraph')) is None:\n",
    "        article_info['subtitle'] = [None]\n",
    "    else:\n",
    "        article_info['subtitle'] = [subtitle.text]\n",
    "    \n",
    "    temp = pd.DataFrame(article_info)\n",
    "    df = pd.concat([df, temp]).reset_index(drop = True)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebaffaa-93b7-4d6b-ad0f-9fb377abc910",
   "metadata": {},
   "source": [
    "A quick check of these results shows that everything is correct with one exception. The date on the last article is not what is shown on the article page. Although when JavaScript is disabled the date is correct. So somehow JavaScript is changing the date after the page loads. A few checks of older articles shows that this is not because the current date is shown by default. It's weird, but for now I will ignore it.\n",
    "\n",
    "### Takeaways\n",
    "\n",
    "Using web scraping with requests and Beautiful Soup can be enough to get all the article information needed. There are some limitations. Each scrape can get at most 10 articles. We are not able to get topic information for each article (we may need to use the API for this purpose and build a model to predict topics). Because we can only scrape 10 articles per publication we may need to run the web scraping script multiple times a day in order to keep up with the latest articles. Otherwise at some point in the future I can explore how to accomplish this with Selenium in order to deal with this issue.\n",
    "\n",
    "## News API\n",
    "\n",
    "I discovered the News API in a Medium article and upon looking through the website it seems that this API could solve a large problem for this project. Namely, this API allows for gathering article data from many different sources, over 80,000 according to the website. While there most likely will be some opportunities to add in other sources through web scraping/APIs using the News API will cover a wide range of online sources that will help to deliver content for the article recommender project. Here in this section we'll look at how this API can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6863ee16-8f72-4829-8a90-6ba0fb5b6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll initialize the News API client using an API key.\n",
    "\n",
    "news = NewsApiClient(api_key = news_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5dbf482-9457-42c6-a2f3-1f830b8255c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if articles from any Medium publications can be pulled from the News API.\n",
    "# According to the documentation the only categories allowed are:\n",
    "# business, entertainment, general, health, science, sports, and technology.\n",
    "\n",
    "sources = news.get_sources(category = 'technology', language = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55d62291-9e3d-43f3-a6e2-6a7839c96610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arstechnica.com',\n",
       " 'https://www.ccn.com',\n",
       " 'https://www.engadget.com',\n",
       " 'https://news.ycombinator.com',\n",
       " 'http://www.recode.net',\n",
       " 'https://techcrunch.com',\n",
       " 'http://www.techradar.com',\n",
       " 'http://thenextweb.com',\n",
       " 'http://www.theverge.com',\n",
       " 'https://www.wired.com']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see only the URLs.\n",
    "\n",
    "[source['url'] for source in sources['sources']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa26167c-1312-4689-bec3-c5c588bee8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try general now.\n",
    "\n",
    "sources = news.get_sources(category = 'general', language = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19a0d49b-b1d2-45c1-8e3c-75b56455d715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://abcnews.go.com',\n",
       " 'http://www.abc.net.au/news',\n",
       " 'http://www.aljazeera.com',\n",
       " 'https://apnews.com/',\n",
       " 'https://www.axios.com',\n",
       " 'http://www.bbc.co.uk/news',\n",
       " 'http://www.breitbart.com',\n",
       " 'http://www.cbc.ca/news',\n",
       " 'http://www.cbsnews.com',\n",
       " 'http://us.cnn.com',\n",
       " 'http://www.foxnews.com',\n",
       " 'https://news.google.com',\n",
       " 'https://news.google.com',\n",
       " 'https://news.google.com',\n",
       " 'https://news.google.com',\n",
       " 'https://news.google.com',\n",
       " 'http://www.independent.co.uk',\n",
       " 'http://www.msnbc.com',\n",
       " 'https://www.nationalreview.com/',\n",
       " 'http://www.nbcnews.com',\n",
       " 'http://www.news24.com',\n",
       " 'http://www.news.com.au',\n",
       " 'https://www.newsweek.com',\n",
       " 'http://nymag.com',\n",
       " 'https://www.politico.com',\n",
       " 'https://www.reddit.com/r/all',\n",
       " 'http://www.reuters.com',\n",
       " 'https://www.rte.ie/news',\n",
       " 'http://www.theamericanconservative.com/',\n",
       " 'https://www.theglobeandmail.com',\n",
       " 'http://thehill.com',\n",
       " 'http://www.thehindu.com',\n",
       " 'http://www.huffingtonpost.com',\n",
       " 'https://www.irishtimes.com',\n",
       " 'https://www.jpost.com/',\n",
       " 'http://timesofindia.indiatimes.com',\n",
       " 'https://www.washingtonpost.com',\n",
       " 'https://www.washingtontimes.com/',\n",
       " 'http://time.com',\n",
       " 'http://www.usatoday.com/news',\n",
       " 'https://news.vice.com']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[source['url'] for source in sources['sources']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1287b7-5444-453c-91dd-9327aba6feb2",
   "metadata": {},
   "source": [
    "From the looks of it the only sources for this API are a lot of the more commonly seen news publications. With that in mind let's now try to pull some data for a specific topic such as data science. We'll also try to get all the same information we got for the Medium articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f465ff0-7226-4f90-9f1e-6d0464649675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll try to get all articles about data science that were published today (July 11, 2022).\n",
    "# For the query it is important to put double quotes around data science, otherwise \"data\" and \"science\" \n",
    "# will be matched separately rather than \"data science\" together.\n",
    "\n",
    "articles = news.get_everything(\n",
    "    q = '\"data science\"',\n",
    "    from_param = '2022-07-11',\n",
    "    language = 'en',\n",
    "    page_size = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "447c62e1-e71c-4ae9-a3c3-de5a3f562bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to Use Mutate function in R',\n",
       " 'Top Posts July 4-10: Free Python Crash Course',\n",
       " \"Steelers' Heinz Field renamed Acrisure Stadium as part of new 15-year naming rights agreement - CBS Sports\",\n",
       " 'Python Engineering at Microsoft: Microsoft at EuroPython 2022',\n",
       " 'Reverse Keyword Warrant Challenged After Cops Asked Google To Search Millions Of People’s Data Multiple Times']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the titles for these articles.\n",
    "\n",
    "[article['title'] for article in articles['articles']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780534e6-a611-40bc-9e82-c275fa05cb24",
   "metadata": {},
   "source": [
    "All of these, with the exception of the third article, seem to be related to data science in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "761b7c15-14ce-416b-a000-24baa1b22923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': {'id': None, 'name': 'R-bloggers.com'},\n",
       " 'author': 'Jim',\n",
       " 'title': 'How to Use Mutate function in R',\n",
       " 'description': 'The post How to Use Mutate function in R appeared first on Data Science Tutorials How to Use Mutate function in R, This article demonstrates how to add additional variables to a data frame using R’s mutate() function. Artificial Intelligence Examples-Quick Vi…',\n",
       " 'url': 'https://www.r-bloggers.com/2022/07/how-to-use-mutate-function-in-r/',\n",
       " 'urlToImage': 'https://www.r-bloggers.com/wp-content/uploads/2016/04/R_02_2016-05-01.png',\n",
       " 'publishedAt': '2022-07-11T08:14:00Z',\n",
       " 'content': '[This article was first published on Data Science Tutorials, and kindly contributed to R-bloggers]. (You can report issue about the content on this page here)\\r\\nWant to share your content on R-blogger… [+9104 chars]'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles['articles'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9325e87d-0b3e-46eb-a9b2-208c219141b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>article_intro</th>\n",
       "      <th>date</th>\n",
       "      <th>read_time</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jim</td>\n",
       "      <td>R-bloggers.com</td>\n",
       "      <td>How to Use Mutate function in R</td>\n",
       "      <td>The post How to Use Mutate function in R appea...</td>\n",
       "      <td>[This article was first published on Data Scie...</td>\n",
       "      <td>2022-07-11T08:14:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.r-bloggers.com/2022/07/how-to-use-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KDnuggets</td>\n",
       "      <td>Kdnuggets.com</td>\n",
       "      <td>Top Posts July 4-10: Free Python Crash Course</td>\n",
       "      <td>Also: 12 Essential VSCode Extensions for Data ...</td>\n",
       "      <td>Also: 12 Essential VSCode Extensions for Data ...</td>\n",
       "      <td>2022-07-11T20:03:23Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.kdnuggets.com/2022/07/top-posts-we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>CBS Sports</td>\n",
       "      <td>Steelers' Heinz Field renamed Acrisure Stadium...</td>\n",
       "      <td>&lt;ol&gt;&lt;li&gt;Steelers' Heinz Field renamed Acrisure...</td>\n",
       "      <td>Heinz Field is no more. Just over two decades ...</td>\n",
       "      <td>2022-07-11T18:28:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.cbssports.com/nfl/news/steelers-he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Luciana Abud</td>\n",
       "      <td>Microsoft.com</td>\n",
       "      <td>Python Engineering at Microsoft: Microsoft at ...</td>\n",
       "      <td>We’re thrilled to be a Platinum Sponsor of Eur...</td>\n",
       "      <td>Luciana Abud\\r\\nJuly 11th, 20220\\r\\nWere thril...</td>\n",
       "      <td>2022-07-11T13:20:09Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://devblogs.microsoft.com/python/microsof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tim Cushing</td>\n",
       "      <td>Techdirt</td>\n",
       "      <td>Reverse Keyword Warrant Challenged After Cops ...</td>\n",
       "      <td>Cops have been running to Google for years, wa...</td>\n",
       "      <td>from the Dragnet-2:-The-Dragnetting dept\\r\\nCo...</td>\n",
       "      <td>2022-07-11T16:38:41Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.techdirt.com/2022/07/11/reverse-ke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author     publication  \\\n",
       "0           Jim  R-bloggers.com   \n",
       "1     KDnuggets   Kdnuggets.com   \n",
       "2                    CBS Sports   \n",
       "3  Luciana Abud   Microsoft.com   \n",
       "4   Tim Cushing        Techdirt   \n",
       "\n",
       "                                               title  \\\n",
       "0                    How to Use Mutate function in R   \n",
       "1      Top Posts July 4-10: Free Python Crash Course   \n",
       "2  Steelers' Heinz Field renamed Acrisure Stadium...   \n",
       "3  Python Engineering at Microsoft: Microsoft at ...   \n",
       "4  Reverse Keyword Warrant Challenged After Cops ...   \n",
       "\n",
       "                                            subtitle  \\\n",
       "0  The post How to Use Mutate function in R appea...   \n",
       "1  Also: 12 Essential VSCode Extensions for Data ...   \n",
       "2  <ol><li>Steelers' Heinz Field renamed Acrisure...   \n",
       "3  We’re thrilled to be a Platinum Sponsor of Eur...   \n",
       "4  Cops have been running to Google for years, wa...   \n",
       "\n",
       "                                       article_intro                  date  \\\n",
       "0  [This article was first published on Data Scie...  2022-07-11T08:14:00Z   \n",
       "1  Also: 12 Essential VSCode Extensions for Data ...  2022-07-11T20:03:23Z   \n",
       "2  Heinz Field is no more. Just over two decades ...  2022-07-11T18:28:00Z   \n",
       "3  Luciana Abud\\r\\nJuly 11th, 20220\\r\\nWere thril...  2022-07-11T13:20:09Z   \n",
       "4  from the Dragnet-2:-The-Dragnetting dept\\r\\nCo...  2022-07-11T16:38:41Z   \n",
       "\n",
       "  read_time                                                url  \n",
       "0      None  https://www.r-bloggers.com/2022/07/how-to-use-...  \n",
       "1      None  https://www.kdnuggets.com/2022/07/top-posts-we...  \n",
       "2      None  https://www.cbssports.com/nfl/news/steelers-he...  \n",
       "3      None  https://devblogs.microsoft.com/python/microsof...  \n",
       "4      None  https://www.techdirt.com/2022/07/11/reverse-ke...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's try to get all the data that we need for each article.\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'author' : [],\n",
    "    'publication' : [],\n",
    "    'title' : [],\n",
    "    'subtitle' : [],\n",
    "    'article_intro' : [],\n",
    "    'date' : [],\n",
    "    'read_time' : [],\n",
    "    'url' : []\n",
    "})\n",
    "\n",
    "for article in articles['articles']:\n",
    "    article_info = {}\n",
    "    \n",
    "    article_info['author'] = [article['author']]\n",
    "    article_info['publication'] = [article['source']['name']]\n",
    "    article_info['title'] = [article['title']]\n",
    "    article_info['subtitle'] = [article['description'][ : 100]]\n",
    "    article_info['article_intro'] = [article['content']]\n",
    "    article_info['date'] = [article['publishedAt']]\n",
    "    article_info['read_time'] = [None]\n",
    "    article_info['url'] = [article['url']]\n",
    "    \n",
    "    temp = pd.DataFrame(article_info)\n",
    "    df = pd.concat([df, temp]).reset_index(drop = True)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f9165-60b1-49c4-a8b1-8ac1610ee353",
   "metadata": {},
   "source": [
    "In order to ensure that all the latest articles are acquired we'll need to go through each page of the return results. By default a page returns 100 articles so we'll need to determine how we can know that we've reached the last page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8750663f-1ed2-4e36-9bf4-c6d79251b1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1, Number of articles: 100\n",
      "Page: 2, Number of articles: 100\n",
      "Page: 3, Number of articles: 100\n",
      "Final Page: 4, Number of articles: 51\n"
     ]
    }
   ],
   "source": [
    "page_number = 1\n",
    "\n",
    "while True:\n",
    "    articles = news.get_everything(\n",
    "        q = '\"machine learning\"',\n",
    "        from_param = '2022-07-27',\n",
    "        language = 'en',\n",
    "        page = page_number\n",
    "    )\n",
    "    \n",
    "    if len(articles['articles']) < 100:\n",
    "        break\n",
    "        \n",
    "    print(f'Page: {page_number}, Number of articles: {len(articles[\"articles\"])}')\n",
    "    \n",
    "    page_number += 1\n",
    "    \n",
    "print(f'Final Page: {page_number}, Number of articles: {len(articles[\"articles\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f82e1-c36b-43d3-ae2f-d1f10aae94c1",
   "metadata": {},
   "source": [
    "As we can see here we're able to determine when we've reached the final page by checking the number of articles returned by the News API. If the number of articles is less than 100 then we have reached the final page.\n",
    "\n",
    "### Takeaways\n",
    "\n",
    "The News API will be very helpful for gathering article data from a very large variety of sources. A quick test (not shown here) shows that articles can be pulled from Medium using the News API, but only a few results will come from Medium and it is dependent on the search query appearing in the article. That being said, I believe the already existing code for gathering article data from Medium publications will be best for that source especially it allows for gathering additional information such as read time. While some of the data acquired from the News API is rather messy, I believe that providing web scraping solutions to gather additional/cleaner data would be far too time consuming of a task and we are better off using what is provided through the News API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "article_recommender_venv",
   "language": "python",
   "name": "article_recommender_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
